#!/usr/bin/env python3
"""
DEPRECATED: This file has been replaced by kokoro_wrapper.py
DO NOT USE in production. Kept for reference only.

This legacy wrapper is no longer maintained. All functionality has been
consolidated into kokoro_wrapper.py with improved offline loading and
unified text chunking via text_chunker.py module.
"""
import os
import sys
import json
import torch
import numpy as np
import io
import re
import soundfile as sf
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

DEFAULT_VOICES = {
    'a': 'af_bella',
    'b': 'bf_emma',
    'e': 'ef_dora',
    'f': 'ff_siwis',
    'h': 'hf_alpha',
    'i': 'if_sara',
    'p': 'pf_dora',
    'j': 'jf_alpha',
    'z': 'zf_ziqi'
}

# Keep chunk size small to stay within Kokoro phoneme limits.
MAX_CHUNK_CHAR_SIZE = 100


def _normalize_lang_code(lang_code: str) -> str:
    normalized = lang_code.lower()
    return ALIASES.get(normalized, normalized) if 'ALIASES' in globals() else normalized


def _default_voice_for_lang(lang_code: str) -> str:
    normalized = _normalize_lang_code(lang_code)
    return DEFAULT_VOICES.get(normalized, 'af_bella')

# æ·»åŠ Kokoroè·¯å¾„åˆ°Pythonè·¯å¾„
def _extend_sys_path() -> None:
    """Ensure Kokoro sources are discoverable regardless of host layout."""
    candidates = []

    repo_override = os.environ.get('KOKORO_REPO_PATH')
    if repo_override:
        for raw_path in repo_override.split(os.pathsep):
            raw_path = raw_path.strip()
            if raw_path:
                candidates.append(Path(raw_path))
                candidates.append(Path(raw_path) / 'kokoro.js')
    else:
        project_root = Path(__file__).resolve().parents[1]
        default_repo = project_root / 'kokoro-main-ref'
        candidates.append(default_repo)
        candidates.append(default_repo / 'kokoro.js')

    additional = os.environ.get('KOKORO_ADDITIONAL_PYTHONPATH')
    if additional:
        for raw_path in additional.split(os.pathsep):
            raw_path = raw_path.strip()
            if raw_path:
                candidates.append(Path(raw_path))

    for candidate in candidates:
        try:
            if candidate.exists():
                resolved = str(candidate.resolve())
                if resolved not in sys.path:
                    sys.path.append(resolved)
        except Exception:
            continue


_extend_sys_path()

# å¯¼å…¥çœŸå®çš„Kokoroæ¨¡å—
try:
    from kokoro.model import KModel
    from kokoro.pipeline import KPipeline
    try:
        # kokoro<0.9.0 re-exported helpers at package root; 0.9.4+ keeps them in pipeline
        from kokoro import ALIASES, LANG_CODES  # type: ignore
    except ImportError:
        from kokoro.pipeline import ALIASES, LANG_CODES
    KOKORO_AVAILABLE = True
except ImportError as e:
    print(f"âŒ Failed to import Kokoro modules: {e}", file=sys.stderr)
    KOKORO_AVAILABLE = False

class KokoroTTSReal:
    def __init__(self):
        self.models = {}
        self.pipelines = {}
        self.device = None
        self.model = None
        
        # è‡ªåŠ¨æ£€æµ‹è®¾å¤‡
        self.setup_device()
        
        # åˆå§‹åŒ–æ¨¡å‹
        if KOKORO_AVAILABLE:
            self.initialize_model()
        
        # å‘é€å°±ç»ªä¿¡å·
        self.send_ready_signal()
        
    def setup_device(self):
        """è®¾ç½®è®¡ç®—è®¾å¤‡ï¼Œä¼˜å…ˆä½¿ç”¨GPU"""
        try:
            # è®¾ç½®ä»£ç†ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if os.environ.get('https_proxy') or os.environ.get('http_proxy'):
                print(f"ğŸŒ Using proxy: {os.environ.get('https_proxy', os.environ.get('http_proxy'))}", file=sys.stderr)

            preferred_device = os.environ.get('KOKORO_DEVICE', 'auto').lower()

            def _use_cuda() -> bool:
                if not torch.cuda.is_available():
                    return False
                self.device = 'cuda'
                gpu_name = torch.cuda.get_device_name(0)
                print(f"ğŸš€ Using GPU: {gpu_name}", file=sys.stderr)
                print(f"ğŸ“Š GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB", file=sys.stderr)
                print(f"ğŸ”¥ CUDA Version: {torch.version.cuda}", file=sys.stderr)
                return True

            def _use_mps() -> bool:
                if not hasattr(torch.backends, 'mps') or not torch.backends.mps.is_available():
                    return False
                self.device = 'mps'
                os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = os.environ.get('PYTORCH_ENABLE_MPS_FALLBACK', '1')
                print("ğŸ Using Apple Silicon MPS", file=sys.stderr)
                return True

            used = False
            if preferred_device == 'cuda':
                used = _use_cuda()
                if not used:
                    print("âš ï¸ Requested CUDA but no compatible GPU is available. Falling back to auto detection.", file=sys.stderr)
            elif preferred_device in {'mps', 'metal'}:
                used = _use_mps()
                if not used:
                    print("âš ï¸ Requested MPS but it is not available. Falling back to auto detection.", file=sys.stderr)
            elif preferred_device == 'cpu':
                self.device = 'cpu'
                print("ğŸ’» Using CPU (forced)", file=sys.stderr)
                used = True

            if not used:
                if _use_cuda():
                    used = True
                elif _use_mps():
                    used = True
                else:
                    self.device = 'cpu'
                    print("ğŸ’» Using CPU (fallback)", file=sys.stderr)

            sys.stderr.flush()

        except Exception as e:
            self.device = 'cpu'
            print(f"âš ï¸ Device detection failed, falling back to CPU: {e}", file=sys.stderr)
            sys.stderr.flush()
    
    def send_ready_signal(self):
        """å‘é€å°±ç»ªä¿¡å·åˆ°stderrï¼Œè®©Node.jsçŸ¥é“æœåŠ¡å·²å‡†å¤‡å¥½"""
        if KOKORO_AVAILABLE and self.model:
            print("ğŸš€ Kokoro TTS service is ready with real GPU acceleration", file=sys.stderr)
        else:
            print("âš ï¸ Kokoro TTS service ready but using fallback mode", file=sys.stderr)
        sys.stderr.flush()
        
    def initialize_model(self):
        """åˆå§‹åŒ–Kokoroæ¨¡å‹"""
        try:
            print("ğŸ”„ Initializing Kokoro model...", file=sys.stderr)
            sys.stderr.flush()
            
            # æ£€æŸ¥æœ¬åœ° .pth æ–‡ä»¶
            local_pth_paths = [
                Path('/app/kokoro-local/.cache/huggingface/hub/models--hexgrad--Kokoro-82M/snapshots/main/kokoro-v1_0.pth'),
                Path.home() / '.cache' / 'huggingface' / 'hub' / 'models--hexgrad--Kokoro-82M' / 'snapshots' / 'main' / 'kokoro-v1_0.pth',
            ]
            
            local_pth = None
            for pth_path in local_pth_paths:
                if pth_path.exists():
                    local_pth = pth_path
                    print(f"âœ… Found local model: {pth_path}", file=sys.stderr)
                    print(f"ğŸ“Š Model size: {pth_path.stat().st_size / 1024 / 1024:.1f} MB", file=sys.stderr)
                    break
            
            if local_pth:
                print("ğŸ“¥ Loading model from local .pth file...", file=sys.stderr)
                sys.stderr.flush()
                
                # è®¾ç½®ç¦»çº¿æ¨¡å¼ç¯å¢ƒå˜é‡ï¼ˆå¼ºåˆ¶ HuggingFace ä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼‰
                os.environ['HF_HUB_OFFLINE'] = '1'
                os.environ['TRANSFORMERS_OFFLINE'] = '1'
                os.environ['HF_DATASETS_OFFLINE'] = '1'
                
                try:
                    # å°è¯•ä»æœ¬åœ°ç¼“å­˜åŠ è½½ï¼ˆä½¿ç”¨ repo_id ä½†å¼ºåˆ¶ç¦»çº¿ï¼‰
                    print("ğŸ“¥ Attempting offline load from cache...", file=sys.stderr)
                    sys.stderr.flush()
                    
                    self.model = KModel(
                        repo_id='hexgrad/Kokoro-82M',
                        disable_complex=False
                    )
                    
                    print("âœ… Model loaded from local cache", file=sys.stderr)
                    sys.stderr.flush()
                    
                except Exception as e:
                    print(f"âš ï¸ Offline load failed: {e}", file=sys.stderr)
                    print("ğŸ“¥ Trying direct weight loading...", file=sys.stderr)
                    sys.stderr.flush()
                    
                    # å›é€€ï¼šç›´æ¥åŠ è½½æƒé‡æ–‡ä»¶
                    # æ³¨æ„ï¼šè¿™å¯èƒ½ä¸å®Œå…¨å…¼å®¹ï¼Œä½†å€¼å¾—ä¸€è¯•
                    state_dict = torch.load(str(local_pth), map_location='cpu')
                    
                    # åˆ›å»ºä¸€ä¸ªç©ºæ¨¡å‹å¹¶åŠ è½½æƒé‡
                    # è¿™é‡Œå‡è®¾ KModel å¯ä»¥æ— å‚æ•°åˆå§‹åŒ–
                    try:
                        self.model = KModel()
                        self.model.load_state_dict(state_dict)
                        print("âœ… Weights loaded directly", file=sys.stderr)
                        sys.stderr.flush()
                    except Exception as e2:
                        print(f"âŒ Direct loading also failed: {e2}", file=sys.stderr)
                        raise Exception(f"Cannot load model: {e}, {e2}")
                        
            else:
                print("ğŸ“¥ Model not found locally, will download from HuggingFace...", file=sys.stderr)
                print("   (This may take 3-5 minutes on first run)", file=sys.stderr)
                sys.stderr.flush()
                
                # ä» HuggingFace ä¸‹è½½
                self.model = KModel(
                    repo_id='hexgrad/Kokoro-82M',
                    disable_complex=False
                )
            
            print("âœ… Model weights loaded", file=sys.stderr)
            sys.stderr.flush()
            
            # ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
            if self.device != 'cpu':
                print(f"ğŸš€ Moving model to {self.device}...", file=sys.stderr)
                sys.stderr.flush()
                self.model = self.model.to(self.device)
                print(f"âœ… Model moved to {self.device}", file=sys.stderr)
                sys.stderr.flush()
                
                # GPUé¢„çƒ­ï¼šè¿è¡Œä¸€æ¬¡å°çš„æ¨ç†
                if self.device == 'cuda':
                    print("ğŸ”¥ Warming up GPU with test inference...", file=sys.stderr)
                    sys.stderr.flush()
                    try:
                        test_pipeline = KPipeline(
                            lang_code='en-us',
                            model=self.model,
                            device=self.device
                        )
                        # è¿è¡Œä¸€ä¸ªå¾ˆçŸ­çš„æµ‹è¯•
                        list(test_pipeline("Hello", voice='af_bella', speed=1.0))
                        print("âœ… GPU warmup complete", file=sys.stderr)
                        sys.stderr.flush()
                    except Exception as warmup_error:
                        print(f"âš ï¸ GPU warmup failed (non-critical): {warmup_error}", file=sys.stderr)
                        sys.stderr.flush()
                
            print(f"âœ… Model initialized on {self.device}", file=sys.stderr)
            sys.stderr.flush()
            
        except Exception as e:
            print(f"âŒ Model initialization failed: {e}", file=sys.stderr)
            sys.stderr.flush()
            self.model = None
    
    def get_pipeline(self, lang_code='en-us', voice='af'):
        """è·å–æˆ–åˆ›å»ºè¯­è¨€ç®¡é“"""
        pipeline_key = f"{lang_code}_{voice}"
        
        if pipeline_key in self.pipelines:
            return self.pipelines[pipeline_key]
            
        try:
            print(f"ğŸ”„ Creating pipeline for {lang_code} with voice {voice}...", file=sys.stderr)
            sys.stderr.flush()
            
            # åˆ›å»ºç®¡é“
            pipeline = KPipeline(
                lang_code=lang_code,
                model=self.model if self.model else True,  # ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹æˆ–è‡ªåŠ¨åˆ›å»º
                device=self.device if self.device != 'cpu' else None
            )
            
            self.pipelines[pipeline_key] = pipeline
            
            print(f"âœ… Pipeline created for {lang_code}_{voice}", file=sys.stderr)
            sys.stderr.flush()
            
            return pipeline
            
        except Exception as e:
            print(f"âŒ Pipeline creation failed for {lang_code}_{voice}: {e}", file=sys.stderr)
            sys.stderr.flush()
            return None

    @staticmethod
    def split_text_intelligently(text: str, max_chunk_size: int = MAX_CHUNK_CHAR_SIZE) -> list[str]:
        """æ™ºèƒ½åˆ†å‰²æ–‡æœ¬ï¼Œä¼˜å…ˆæŒ‰ç…§æ®µè½ä¸å¥å­è¾¹ç•Œæ‹†åˆ†é•¿æ–‡æœ¬ã€‚"""
        chunks: list[str] = []
        paragraphs = text.split('\n\n')
        current_chunk = ''

        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                continue

            if len(current_chunk) + len(paragraph) <= max_chunk_size:
                current_chunk += (paragraph + '\n\n')
                continue

            if current_chunk.strip():
                chunks.append(current_chunk.strip())
                current_chunk = ''

            if len(paragraph) > max_chunk_size:
                chunks.extend(KokoroTTSReal.split_by_sentences(paragraph, max_chunk_size))
            else:
                current_chunk = paragraph + '\n\n'

        if current_chunk.strip():
            chunks.append(current_chunk.strip())

        return chunks or [text]

    @staticmethod
    def split_by_sentences(text: str, max_chunk_size: int = MAX_CHUNK_CHAR_SIZE) -> list[str]:
        sentences = re.split(r'(?<=[.!?])\s+', text)
        chunks: list[str] = []
        current_chunk = ''

        for sentence in sentences:
            if len(current_chunk) + len(sentence) <= max_chunk_size:
                current_chunk += (sentence + ' ')
                continue

            if current_chunk.strip():
                chunks.append(current_chunk.strip())
                current_chunk = ''

            if len(sentence) > max_chunk_size:
                chunks.extend(KokoroTTSReal.split_by_commas(sentence, max_chunk_size))
            else:
                current_chunk = sentence + ' '

        if current_chunk.strip():
            chunks.append(current_chunk.strip())

        return chunks

    @staticmethod
    def split_by_commas(text: str, max_chunk_size: int = MAX_CHUNK_CHAR_SIZE) -> list[str]:
        parts = text.split(', ')
        chunks: list[str] = []
        current_chunk = ''

        for part in parts:
            if len(current_chunk) + len(part) <= max_chunk_size:
                current_chunk += (part + ', ')
                continue

            if current_chunk.strip():
                chunks.append(current_chunk.rstrip(', '))
                current_chunk = ''

            if len(part) > max_chunk_size:
                remaining = part
                while len(remaining) > max_chunk_size:
                    chunks.append(remaining[:max_chunk_size])
                    remaining = remaining[max_chunk_size:]
                if remaining:
                    current_chunk = remaining + ', '
            else:
                current_chunk = part + ', '

        if current_chunk.strip():
            chunks.append(current_chunk.rstrip(', '))

        return chunks
    
    def synthesize_audio(self, text, voice='af', speed=1.0, lang_code='en-us'):
        """ä½¿ç”¨çœŸå®Kokoroæ¨¡å‹åˆæˆéŸ³é¢‘"""
        try:
            if not KOKORO_AVAILABLE:
                raise Exception("Kokoro modules not available")
                
            if not self.model:
                raise Exception("Model not initialized")
            
            print(f"ğŸ¤ Synthesizing audio: {text[:50]}...", file=sys.stderr)
            print(f"ğŸ­ Voice request: {voice}, Language: {lang_code}, Speed: {speed}", file=sys.stderr)
            sys.stderr.flush()
            
            # è·å–ç®¡é“
            pipeline = self.get_pipeline(lang_code, voice)
            if not pipeline:
                raise Exception(f"Failed to create pipeline for {lang_code}")
            
            pipeline_lang = getattr(pipeline, 'lang_code', lang_code)
            requested_voice = voice or _default_voice_for_lang(pipeline_lang)

            # ç¡®ä¿è¯­éŸ³åŒ…å¯ç”¨ï¼Œå¿…è¦æ—¶è‡ªåŠ¨å›é€€
            def _ensure_voice(voice_id: str) -> str:
                try:
                    pipeline.load_voice(voice_id)
                    return voice_id
                except Exception as voice_error:
                    fallback_voice = _default_voice_for_lang(pipeline_lang)
                    if fallback_voice != voice_id:
                        print(f"ğŸ”„ Voice fallback: {voice_id} â†’ {fallback_voice} ({voice_error})", file=sys.stderr)
                        sys.stderr.flush()
                        pipeline.load_voice(fallback_voice)
                        return fallback_voice
                    raise

            requested_voice = _ensure_voice(requested_voice)

            # éå†ç®¡é“è¾“å‡ºï¼Œæ”¶é›†æ‰€æœ‰éŸ³é¢‘æ®µ
            audio_segments: list[torch.Tensor] = []
            segment_count = 0

            def _extract_audio(segment):
                if hasattr(segment, 'audio'):
                    return segment.audio, getattr(segment, 'graphemes', None)
                if isinstance(segment, (list, tuple)) and len(segment) >= 3:
                    return segment[2], segment[0]
                return None, None

            def _collect_segments(input_text: str, chunk_index: int | None = None) -> None:
                nonlocal segment_count
                for segment in pipeline(
                    input_text,
                    voice=requested_voice,
                    speed=speed,
                    split_pattern=r'\n+'
                ):
                    audio_tensor, graphemes = _extract_audio(segment)
                    if audio_tensor is None:
                        continue
                    if isinstance(audio_tensor, torch.Tensor):
                        audio_tensor = audio_tensor.detach().to('cpu')
                    else:
                        audio_tensor = torch.tensor(audio_tensor, dtype=torch.float32)
                    audio_segments.append(audio_tensor)
                    segment_count += 1
                    segment_info = len(graphemes) if isinstance(graphemes, str) else 'unknown'
                    prefix = f"chunk {chunk_index+1} " if chunk_index is not None else ''
                    print(f"âœ… Segment {segment_count} captured ({prefix}chars: {segment_info})", file=sys.stderr)

            if len(text) > MAX_CHUNK_CHAR_SIZE:
                chunks = self.split_text_intelligently(text)
                print(f"ğŸ§© Split text into {len(chunks)} chunks for processing", file=sys.stderr)
                for idx, chunk_text in enumerate(chunks):
                    print(f"ğŸš€ Processing chunk {idx+1}/{len(chunks)} ({len(chunk_text)} chars)", file=sys.stderr)
                    _collect_segments(chunk_text, idx)
            else:
                _collect_segments(text)

            if not audio_segments:
                raise Exception("No audio data generated")

            # æ‹¼æ¥æ‰€æœ‰éŸ³é¢‘æ®µ
            combined_audio = torch.cat(audio_segments)

            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            audio_array = combined_audio.cpu().numpy() if hasattr(combined_audio, 'cpu') else np.array(combined_audio)

            # ç¡®ä¿éŸ³é¢‘æ•°æ®æ˜¯float32æ ¼å¼
            if audio_array.dtype != np.float32:
                audio_array = audio_array.astype(np.float32)

            # å½’ä¸€åŒ–éŸ³é¢‘åˆ°[-1, 1]èŒƒå›´
            max_val = np.abs(audio_array).max()
            if max_val > 0:
                audio_array = audio_array / max_val * 0.95  # ç•™ä¸€äº›ä½™é‡é¿å…å‰Šæ³¢

            # Kokoroé»˜è®¤é‡‡æ ·ç‡æ˜¯24000Hz
            sample_rate = 24000

            print(f"ğŸµ Audio generated: {len(audio_array)} samples at {sample_rate}Hz (segments: {segment_count})", file=sys.stderr)
            sys.stderr.flush()

            # åˆ›å»ºWAVæ–‡ä»¶ï¼ˆä¿æŒPCM16ä»¥å…¼å®¹å‰ç«¯æ’­æ”¾å™¨ï¼‰
            try:
                buffer = io.BytesIO()
                sf.write(buffer, audio_array, sample_rate, format='WAV', subtype='PCM_16')
                audio_bytes = buffer.getvalue()
                audio_hex = audio_bytes.hex()

                print(f"âœ… Audio synthesized successfully: {len(audio_hex)} hex chars", file=sys.stderr)
                print(f"ğŸ“Š Audio size: {len(audio_bytes)} bytes (voice: {requested_voice}, segments: {segment_count})", file=sys.stderr)
                sys.stderr.flush()

                return audio_hex

            except Exception as wav_error:
                raise Exception(f"Failed to encode WAV audio: {wav_error}")
                
        except Exception as e:
            print(f"âŒ Audio synthesis failed: {e}", file=sys.stderr)
            sys.stderr.flush()
            raise Exception(f"Audio synthesis failed: {e}")
    
    def process_request(self, request_data):
        """å¤„ç†JSONè¯·æ±‚"""
        try:
            request = json.loads(request_data)
            
            text = request.get('text', '')
            speed = request.get('speed', 1.0)
            voice = request.get('voice', 'af')
            lang_code = request.get('lang_code', 'en-us')
            
            if not text:
                return {
                    'success': False,
                    'error': 'Text cannot be empty'
                }
            
            # åˆæˆéŸ³é¢‘
            audio_data = self.synthesize_audio(text, voice, speed, lang_code)
            
            return {
                'success': True,
                'audio_data': audio_data,
                'device': self.device,
                'message': f'Audio synthesized using {self.device} with voice: {voice}'
            }
            
        except json.JSONDecodeError:
            return {
                'success': False,
                'error': 'Invalid JSON request'
            }
        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }
    
    def run(self):
        """è¿è¡Œäº¤äº’å¼æœåŠ¡"""
        while True:
            try:
                # è¯»å–è¾“å…¥
                line = sys.stdin.readline()
                if not line:
                    break
                    
                line = line.strip()
                if not line:
                    continue
                
                # å¤„ç†è¯·æ±‚
                response = self.process_request(line)
                
                # å‘é€å“åº”
                print(json.dumps(response), flush=True)
                
            except KeyboardInterrupt:
                break
            except Exception as e:
                error_response = {
                    'success': False,
                    'error': f'Service error: {e}'
                }
                print(json.dumps(error_response), flush=True)

def main():
    """ä¸»å‡½æ•°"""
    try:
        service = KokoroTTSReal()
        service.run()
    except Exception as e:
        print(f"Failed to start Kokoro TTS service: {e}", file=sys.stderr)
        sys.exit(1)

if __name__ == '__main__':
    main()
