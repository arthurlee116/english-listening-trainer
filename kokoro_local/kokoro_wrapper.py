#!/usr/bin/env python3
"""
Kokoro TTS Wrapper for Apple Silicon M4/Metal Acceleration
æœ¬åœ°TTSæœåŠ¡åŒ…è£…å™¨ï¼Œæ”¯æŒMetalåŠ é€Ÿå’Œæ¨¡åž‹é¢„åŠ è½½
"""

import asyncio
import json
import sys
import os
import logging
from threading import Lock
from typing import Optional
import torch
import soundfile as sf
import io
from text_chunker import split_text_intelligently, MAX_CHUNK_CHAR_SIZE

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.WARNING)
logger = logging.getLogger(__name__)

# è®¾ç½®äº‹ä»¶å¾ªçŽ¯ç­–ç•¥ä»¥é¿å…macOSä¸Šçš„é—®é¢˜
if sys.platform == "darwin":
    import asyncio
    if sys.version_info >= (3, 8):
        asyncio.set_event_loop_policy(asyncio.DefaultEventLoopPolicy())

class KokoroTTSWrapper:
    """Kokoro TTSæœåŠ¡åŒ…è£…å™¨ï¼Œæ”¯æŒåŠ¨æ€è¯­è¨€é…ç½®"""
    
    def __init__(self, lang_code: str = 'a', voice: str = 'af_heart'):
        self.pipeline = None
        self.device = None
        self.voice_pack = None
        self.initialized = False
        self.lang_code = lang_code
        self.voice = voice
        self.current_lang_code = None
        self.current_voice = None
        self._pipeline_lock = Lock()
        
    def setup_device(self):
        """è®¾ç½®è®¡ç®—è®¾å¤‡ (è‡ªåŠ¨æ£€æµ‹CUDA/Metal/CPU)"""
        try:
            # èŽ·å–çŽ¯å¢ƒå˜é‡æŒ‡å®šçš„è®¾å¤‡
            device_override = os.environ.get('KOKORO_DEVICE', 'auto').lower()
            
            if device_override == 'cuda' and torch.cuda.is_available():
                self.device = 'cuda'
                print(f"ðŸš€ Using CUDA acceleration (GPU: {torch.cuda.get_device_name(0)})", file=sys.stderr)
                return
            elif device_override == 'metal' and torch.backends.mps.is_available():
                self.device = 'mps'
                os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
                print("ðŸš€ Using Metal Performance Shaders (MPS) for Apple Silicon", file=sys.stderr)
                return
            elif device_override == 'cpu':
                self.device = 'cpu'
                print("ðŸ“± Using CPU for TTS processing (forced)", file=sys.stderr)
                return
            
            # è‡ªåŠ¨æ£€æµ‹æœ€ä½³è®¾å¤‡
            if torch.cuda.is_available():
                # NVIDIA CUDA æ”¯æŒ
                self.device = 'cuda'
                gpu_name = torch.cuda.get_device_name(0)
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                print(f"ðŸš€ Using CUDA acceleration (GPU: {gpu_name}, Memory: {gpu_memory:.1f}GB)", file=sys.stderr)
                # è®¾ç½®CUDAä¼˜åŒ–
                torch.backends.cudnn.benchmark = True
            elif torch.backends.mps.is_available():
                # Apple Silicon Metal æ”¯æŒ
                self.device = 'mps'
                os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
                print("ðŸš€ Using Metal Performance Shaders (MPS) for Apple Silicon", file=sys.stderr)
            else:
                # CPU åŽå¤‡
                self.device = 'cpu'
                print("ðŸ“± Using CPU for TTS processing", file=sys.stderr)
        except Exception as e:
            print(f"ERROR in setup_device: {e}", file=sys.stderr)
            self.device = 'cpu'  # å›žé€€åˆ°CPU
            
    async def initialize(self, lang_code: Optional[str] = None, voice: Optional[str] = None):
        """åˆå§‹åŒ–æˆ–é‡æ–°åˆå§‹åŒ–Kokoroæ¨¡åž‹å’Œè¯­éŸ³"""
        # æ›´æ–°è¯­è¨€å’Œè¯­éŸ³é…ç½®
        if lang_code is not None:
            self.lang_code = lang_code
        if voice is not None:
            self.voice = voice
            
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°åˆå§‹åŒ–
        need_reinit = (
            not self.initialized or 
            self.current_lang_code != self.lang_code or
            self.current_voice != self.voice
        )
        
        if not need_reinit:
            return
            
        try:
            self.setup_device()
            
            if not self.device:
                raise Exception("Failed to setup device")
            
            # å¯¼å…¥Kokoro (å»¶è¿Ÿå¯¼å…¥ä»¥å¤„ç†å¯èƒ½çš„ä¾èµ–é—®é¢˜)
            from kokoro.pipeline import KPipeline
            
            # ç¦ç”¨kokoroå†…éƒ¨æ—¥å¿—
            import kokoro
            kokoro.logger.disable("kokoro")
            
            # åˆå§‹åŒ–pipeline (å¦‚æžœè¯­è¨€æ”¹å˜éœ€è¦é‡æ–°åˆ›å»º)
            if self.current_lang_code != self.lang_code:
                print(f"ðŸŒ Initializing pipeline for language: {self.lang_code}", file=sys.stderr)

                # å¼ºåˆ¶ç¦»çº¿æ¨¡å¼ï¼Œä¸å…è®¸åœ¨çº¿ä¸‹è½½
                os.environ['HF_HUB_OFFLINE'] = '1'
                os.environ['TRANSFORMERS_OFFLINE'] = '1'

                # æ‰«ææœ¬åœ°æ¨¡åž‹è·¯å¾„ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
                from pathlib import Path
                import shutil

                local_model_paths = []
                # ä¼˜å…ˆï¼šçŽ¯å¢ƒå˜é‡æŒ‡å®šçš„è·¯å¾„ï¼ˆä»…åœ¨è®¾ç½®ä¸”éžç©ºæ—¶æ·»åŠ ï¼‰
                env_model_path = os.environ.get('KOKORO_LOCAL_MODEL_PATH')
                if env_model_path:
                    local_model_paths.append(Path(env_model_path))

                # æ¬¡é€‰å’Œå¤‡é€‰è·¯å¾„
                local_model_paths.extend([
                    # æ¬¡é€‰ï¼šé¡¹ç›®æœ¬åœ°ç¼“å­˜
                    Path('kokoro_local/.cache/huggingface/hub/models--hexgrad--Kokoro-82M/snapshots/main'),
                    # å¤‡é€‰ï¼šç”¨æˆ· home ç›®å½•ç¼“å­˜
                    Path.home() / '.cache/huggingface/hub/models--hexgrad--Kokoro-82M/snapshots/main',
                    # å¤‡é€‰ï¼šç‹¬ç«‹æ¨¡åž‹ç›®å½•
                    Path('kokoro-models/Kokoro-82M'),
                ])

                found_model = None
                for model_path in local_model_paths:
                    if model_path.exists() and model_path.is_dir():
                        found_model = model_path
                        print(f"âœ… Found local model at: {model_path}", file=sys.stderr)
                        break

                if not found_model:
                    # æ¨¡åž‹æœªæ‰¾åˆ°ï¼ŒæŠ›å‡ºæ˜Žç¡®é”™è¯¯
                    error_msg = (
                        "âŒ Kokoro model not found in offline mode.\n"
                        "Searched paths:\n" +
                        "\n".join([f"  - {p}" for p in local_model_paths if str(p)]) +
                        "\n\nPlease ensure model exists at one of the above paths,\n"
                        "or set KOKORO_LOCAL_MODEL_PATH environment variable."
                    )
                    raise FileNotFoundError(error_msg)

                # ç¡®ä¿æ¨¡åž‹åœ¨æ ‡å‡†ç¼“å­˜ä½ç½®ï¼ˆKokoro æœŸæœ›çš„ç»“æž„ï¼‰
                cache_path = Path.home() / '.cache/huggingface/hub/models--hexgrad--Kokoro-82M'
                snapshot_dir = cache_path / 'snapshots/main'

                if not snapshot_dir.exists() and found_model != snapshot_dir:
                    print(f"ðŸ“¦ Copying model to HuggingFace cache structure...", file=sys.stderr)
                    cache_path.mkdir(parents=True, exist_ok=True)
                    (cache_path / 'refs').mkdir(exist_ok=True)
                    shutil.copytree(found_model, snapshot_dir, dirs_exist_ok=True)
                    (cache_path / 'refs/main').write_text('main')
                    print("âœ… Model copied to cache", file=sys.stderr)

                # åˆå§‹åŒ– pipelineï¼ˆä»…ç¦»çº¿æ¨¡å¼ï¼‰
                try:
                    self.pipeline = KPipeline(lang_code=self.lang_code)
                    print(f"âœ… Kokoro TTS initialized in offline mode for language: {self.lang_code}", file=sys.stderr)
                except Exception as e:
                    print(f"âŒ Pipeline initialization failed: {e}", file=sys.stderr)
                    raise RuntimeError(f"Failed to initialize Kokoro pipeline: {e}")

                self.current_lang_code = self.lang_code
            
            # åŠ è½½è¯­éŸ³ (å¦‚æžœè¯­éŸ³æ”¹å˜éœ€è¦é‡æ–°åŠ è½½)
            if self.current_voice != self.voice:
                try:
                    print(f"ðŸŽµ Loading voice: {self.voice}", file=sys.stderr)
                    self.voice_pack = self.pipeline.load_voice(self.voice)
                    self.current_voice = self.voice
                except Exception as e:
                    print(f"âš ï¸  Warning: Failed to load voice {self.voice}: {e}", file=sys.stderr)
                    # å°è¯•åŠ è½½é»˜è®¤è¯­éŸ³
                    try:
                        fallback_voice = 'af_heart' if self.lang_code == 'a' else f"{self.lang_code}f_alpha"
                        print(f"ðŸ”„ Trying fallback voice: {fallback_voice}", file=sys.stderr)
                        self.voice_pack = self.pipeline.load_voice(fallback_voice)
                        self.voice = fallback_voice
                        self.current_voice = fallback_voice
                    except Exception as e2:
                        print(f"âŒ Fallback voice also failed: {e2}", file=sys.stderr)
                        self.voice_pack = None
            
            self.initialized = True
            
            # å‘é€å°±ç»ªä¿¡å·åˆ°stderr
            print(f'ðŸš€ Kokoro TTS service ready - Language: {self.lang_code}, Voice: {self.voice}', file=sys.stderr)
            sys.stderr.flush()
            
        except Exception as e:
            print(f"âŒ Initialization failed: {e}", file=sys.stderr)
            raise
            
    async def generate_speech(self, text: str, speed: float = 1.0, parallel: bool = True) -> str:
        """ç”ŸæˆéŸ³é¢‘ï¼Œè¿”å›žåå…­è¿›åˆ¶å­—ç¬¦ä¸²"""
        if not self.initialized:
            await self.initialize()
            
        if not text or not text.strip():
            raise ValueError("Text cannot be empty")
        
        # å¤„ç†é•¿æ–‡æœ¬ï¼šåˆ†å—è€Œä¸æ˜¯æˆªå–
        print(f"ðŸ” Text length check: {len(text)} chars", file=sys.stderr)
        if len(text) > MAX_CHUNK_CHAR_SIZE:
            print(f"ðŸ“ Text is long ({len(text)} chars), will chunk into {MAX_CHUNK_CHAR_SIZE}-char pieces", file=sys.stderr)
            return await self.generate_speech_chunked(text, speed)
            
        # å¯¹äºŽçŸ­æ–‡æœ¬ï¼Œç›´æŽ¥ä½¿ç”¨å•å—å¤„ç†
        return await self.generate_speech_single(text, speed)

    async def generate_speech_chunked(self, text: str, speed: float = 1.0) -> str:
        """åˆ†å—ç”ŸæˆéŸ³é¢‘å¹¶æ‹¼æŽ¥ï¼ˆä¸²è¡Œä¿éšœ pipeline å®‰å…¨ï¼Œä½¿ç”¨ soundfile è¾“å‡ºè§„èŒƒ WAVï¼‰"""
        chunks = split_text_intelligently(text)
        total = len(chunks)
        print(f"ðŸ§© Split text into {total} chunks for processing", file=sys.stderr)

        lock = self._pipeline_lock

        def _synthesize_chunk(chunk_text: str, s: float):
            import numpy as np
            audio_tensors = []
            with lock:
                for _, _, audio in self.pipeline(
                    chunk_text,
                    voice=self.voice,
                    speed=s,
                    split_pattern=r'\n+'
                ):
                    if audio is not None:
                        if self.device == 'mps':
                            audio = audio.to('cpu')
                        audio_tensors.append(audio.detach().cpu())

            if not audio_tensors:
                raise RuntimeError("No audio chunks were generated")

            combined_tensor = torch.cat(audio_tensors)
            return combined_tensor.numpy().astype(np.float32)

        pcm_segments = []
        for idx, chunk in enumerate(chunks):
            print(f"ðŸš€ Synthesizing chunk {idx + 1}/{total}: {len(chunk)} chars", file=sys.stderr)
            segment = await asyncio.to_thread(_synthesize_chunk, chunk, speed)
            pcm_segments.append(segment)

        if not pcm_segments:
            raise RuntimeError("No PCM segments collected from chunk synthesis")

        import numpy as np
        concatenated = np.concatenate(pcm_segments)
        buffer = io.BytesIO()
        sf.write(buffer, concatenated, 24000, format='WAV', subtype='PCM_16')
        total_duration = concatenated.shape[0] / 24000
        print(f"ðŸŽµ Combined {len(pcm_segments)} chunks, total length: {total_duration:.2f}s", file=sys.stderr)
        return buffer.getvalue().hex()
    
    async def generate_speech_single(self, text: str, speed: float = 1.0) -> str:
        """ç”Ÿæˆå•ä¸ªæ–‡æœ¬å—çš„éŸ³é¢‘ï¼ˆä¸²è¡Œç”Ÿæˆï¼Œç¡®ä¿ pipeline å®‰å…¨ï¼‰"""
        try:
            def _synthesize_single():
                import numpy as np
                audio_segments = []
                with self._pipeline_lock:
                    for _, _, audio in self.pipeline(
                        text,
                        voice=self.voice,
                        speed=speed,
                        split_pattern=r'\n+'
                    ):
                        if audio is not None:
                            if self.device == 'mps':
                                audio = audio.to('cpu')
                            audio_segments.append(audio.detach().cpu())

                if not audio_segments:
                    raise RuntimeError("No audio chunks were generated")

                combined = torch.cat(audio_segments)
                return combined.numpy().astype(np.float32)

            pcm = await asyncio.to_thread(_synthesize_single)
            buffer = io.BytesIO()
            sf.write(buffer, pcm, 24000, format='WAV', subtype='PCM_16')
            total_duration = pcm.shape[0] / 24000
            print(f"ðŸŽµ Generated single segment, total length: {total_duration:.2f}s", file=sys.stderr)
            return buffer.getvalue().hex()

        except Exception as e:
            print(f"âŒ Error in generate_speech_single: {e}", file=sys.stderr)
            import traceback
            traceback.print_exc()
            raise

async def main():
    """ä¸»å‡½æ•°ï¼šç›‘å¬stdinè¾“å…¥å¹¶å¤„ç†è¯·æ±‚"""
    service = KokoroTTSWrapper()
    
    try:
        # åˆå§‹åŒ–æ¨¡åž‹
        await service.initialize()
        
        # å‘é€å°±ç»ªä¿¡å·åˆ°stderr
        print('ðŸš€ Kokoro TTS service is ready', file=sys.stderr)
        sys.stderr.flush()
        
        # ç›‘å¬æ ‡å‡†è¾“å…¥
        while True:
            try:
                line = sys.stdin.readline()
                if not line:
                    break
                    
                line = line.strip()
                if not line:
                    continue
                    
                # è§£æžJSONè¯·æ±‚
                request = json.loads(line)
                
                # èŽ·å–è¯­è¨€å’Œè¯­éŸ³é…ç½®
                lang_code = request.get('lang_code', 'a')
                voice = request.get('voice', 'af_heart')
                
                # é‡æ–°åˆå§‹åŒ–æœåŠ¡ï¼ˆå¦‚æžœéœ€è¦ï¼‰
                await service.initialize(lang_code=lang_code, voice=voice)
                
                # å¤„ç†è¯·æ±‚
                result = await service.generate_speech(
                    text=request.get('text', ''),
                    speed=float(request.get('speed', 1.0))
                )
                
                # è¿”å›žç»“æžœ
                response = {
                    "success": True,
                    "audio_data": result,
                    "device": service.device or "unknown",
                    "lang_code": service.lang_code,
                    "voice": service.voice,
                    "message": "Audio generated successfully"
                }
                
                # å‘é€JSONå“åº”
                json_response = json.dumps(response)
                print(json_response, flush=True)
                
            except json.JSONDecodeError:
                error_response = {
                    "success": False,
                    "error": "Invalid JSON format"
                }
                print(json.dumps(error_response))
                sys.stdout.flush()
                
            except Exception as e:
                error_response = {
                    "success": False,
                    "error": str(e)
                }
                print(json.dumps(error_response))
                sys.stdout.flush()
                
    except KeyboardInterrupt:
        pass
    except Exception as e:
        sys.exit(1)

# ç®€å•æµ‹è¯•å‡½æ•°
async def test():
    """æµ‹è¯•å‡½æ•°"""
    service = KokoroTTSWrapper()
    await service.initialize()
    result = await service.generate_speech("Hello test", 1.0)
    print(json.dumps({
        "success": True,
        "audio_data": result,
        "device": service.device or "unknown",
        "message": "Test completed"
    }))

if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1 and sys.argv[1] == "--test":
        asyncio.run(test())
    else:
        asyncio.run(main())
