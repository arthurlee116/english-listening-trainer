#!/usr/bin/env python3
"""
Kokoro TTS Wrapper for Apple Silicon M4/Metal Acceleration
æœ¬åœ°TTSæœåŠ¡åŒ…è£…å™¨ï¼Œæ”¯æŒMetalåŠ é€Ÿå’Œæ¨¡å‹é¢„åŠ è½½
"""

import asyncio
import io
import json
import logging
import os
import sys
import warnings
from typing import Optional

import soundfile as sf
import torch

from text_chunker import MAX_CHUNK_CHAR_SIZE, split_text_intelligently

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.WARNING)
logger = logging.getLogger(__name__)

# è®¾ç½®äº‹ä»¶å¾ªç¯ç­–ç•¥ä»¥é¿å…macOSä¸Šçš„é—®é¢˜
if sys.platform == "darwin":
    import asyncio
    if sys.version_info >= (3, 8):
        asyncio.set_event_loop_policy(asyncio.DefaultEventLoopPolicy())

class KokoroTTSWrapper:
    """Kokoro TTSæœåŠ¡åŒ…è£…å™¨ï¼Œæ”¯æŒåŠ¨æ€è¯­è¨€é…ç½®"""
    
    def __init__(self, lang_code: str = 'a', voice: str = 'af_heart'):
        self.pipeline = None
        self.device = None
        self.voice_pack = None
        self.initialized = False
        self.lang_code = lang_code
        self.voice = voice
        self.current_lang_code = None
        self.current_voice = None
        self._gpu_fallback_attempted = False
        self._warned_about_known_warnings = False

        # ä»…è¾“å‡ºä¸€æ¬¡æ¥è‡ªä¸Šæ¸¸çš„å·²çŸ¥ PyTorch è­¦å‘Šï¼Œé¿å…å™ªå£°
        warnings.filterwarnings(
            "once",
            message="dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1",
            category=UserWarning,
        )
        warnings.filterwarnings(
            "once",
            message="torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.",
            category=UserWarning,
        )
        
    def setup_device(self):
        """è®¾ç½®è®¡ç®—è®¾å¤‡ (è‡ªåŠ¨æ£€æµ‹CUDA/Metal/CPU)"""
        try:
            if self._gpu_fallback_attempted:
                self.device = 'cpu'
                print("âš ï¸  GPU fallback previously triggered; forcing CPU execution for stability.", file=sys.stderr)
                return

            # è·å–ç¯å¢ƒå˜é‡æŒ‡å®šçš„è®¾å¤‡
            device_override = os.environ.get('KOKORO_DEVICE', 'auto').lower()
            
            if device_override == 'cuda' and torch.cuda.is_available():
                if self._is_cuda_arch_supported():
                    self.device = 'cuda'
                    print(f"ğŸš€ Using CUDA acceleration (GPU: {torch.cuda.get_device_name(0)})", file=sys.stderr)
                    return
                print("âš ï¸  CUDA capability not supported by this PyTorch build, falling back to CPU.", file=sys.stderr)
                self._gpu_fallback_attempted = True
                self.device = 'cpu'
                return
            elif device_override == 'metal' and torch.backends.mps.is_available():
                self.device = 'mps'
                os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
                print("ğŸš€ Using Metal Performance Shaders (MPS) for Apple Silicon", file=sys.stderr)
                return
            elif device_override == 'cpu':
                self.device = 'cpu'
                print("ğŸ“± Using CPU for TTS processing (forced)", file=sys.stderr)
                return
            
            # è‡ªåŠ¨æ£€æµ‹æœ€ä½³è®¾å¤‡
            if torch.cuda.is_available():
                # NVIDIA CUDA æ”¯æŒ
                if self._is_cuda_arch_supported():
                    self.device = 'cuda'
                    gpu_name = torch.cuda.get_device_name(0)
                    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                    print(f"ğŸš€ Using CUDA acceleration (GPU: {gpu_name}, Memory: {gpu_memory:.1f}GB)", file=sys.stderr)
                    # è®¾ç½®CUDAä¼˜åŒ–
                    torch.backends.cudnn.benchmark = True
                else:
                    self._gpu_fallback_attempted = True
                    self.device = 'cpu'
                    print("âš ï¸  Detected CUDA device lacks compiled kernels in this PyTorch build; using CPU instead.", file=sys.stderr)
            elif torch.backends.mps.is_available():
                # Apple Silicon Metal æ”¯æŒ
                self.device = 'mps'
                os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
                print("ğŸš€ Using Metal Performance Shaders (MPS) for Apple Silicon", file=sys.stderr)
            else:
                # CPU åå¤‡
                self.device = 'cpu'
                print("ğŸ“± Using CPU for TTS processing", file=sys.stderr)
        except Exception as e:
            print(f"ERROR in setup_device: {e}", file=sys.stderr)
            self.device = 'cpu'  # å›é€€åˆ°CPU
            
    async def initialize(self, lang_code: Optional[str] = None, voice: Optional[str] = None):
        """åˆå§‹åŒ–æˆ–é‡æ–°åˆå§‹åŒ–Kokoroæ¨¡å‹å’Œè¯­éŸ³"""
        # æ›´æ–°è¯­è¨€å’Œè¯­éŸ³é…ç½®
        if lang_code is not None:
            self.lang_code = lang_code
        if voice is not None:
            self.voice = voice
            
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°åˆå§‹åŒ–
        need_reinit = (
            not self.initialized or 
            self.current_lang_code != self.lang_code or
            self.current_voice != self.voice
        )
        
        if not need_reinit:
            return
            
        try:
            self.setup_device()
            
            if not self.device:
                raise Exception("Failed to setup device")
            
            # å¯¼å…¥Kokoro (å»¶è¿Ÿå¯¼å…¥ä»¥å¤„ç†å¯èƒ½çš„ä¾èµ–é—®é¢˜)
            from kokoro.pipeline import KPipeline
            
            # ç¦ç”¨kokoroå†…éƒ¨æ—¥å¿—
            import kokoro
            kokoro.logger.disable("kokoro")
            self._log_known_warning_context()
            
            # åˆå§‹åŒ–pipeline (å¦‚æœè¯­è¨€æ”¹å˜éœ€è¦é‡æ–°åˆ›å»º)
            if self.current_lang_code != self.lang_code:
                print(f"ğŸŒ Initializing pipeline for language: {self.lang_code}", file=sys.stderr)

                # å¼ºåˆ¶ç¦»çº¿æ¨¡å¼ï¼Œä¸å…è®¸åœ¨çº¿ä¸‹è½½
                os.environ['HF_HUB_OFFLINE'] = '1'
                os.environ['TRANSFORMERS_OFFLINE'] = '1'

                # æ‰«ææœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
                from pathlib import Path
                import shutil

                local_model_paths = []
                # ä¼˜å…ˆï¼šç¯å¢ƒå˜é‡æŒ‡å®šçš„è·¯å¾„ï¼ˆä»…åœ¨è®¾ç½®ä¸”éç©ºæ—¶æ·»åŠ ï¼‰
                env_model_path = os.environ.get('KOKORO_LOCAL_MODEL_PATH')
                if env_model_path:
                    local_model_paths.append(Path(env_model_path))

                # æ¬¡é€‰å’Œå¤‡é€‰è·¯å¾„
                local_model_paths.extend([
                    # æ¬¡é€‰ï¼šé¡¹ç›®æœ¬åœ°ç¼“å­˜
                    Path('kokoro_local/.cache/huggingface/hub/models--hexgrad--Kokoro-82M/snapshots/main'),
                    # å¤‡é€‰ï¼šç”¨æˆ· home ç›®å½•ç¼“å­˜
                    Path.home() / '.cache/huggingface/hub/models--hexgrad--Kokoro-82M/snapshots/main',
                    # å¤‡é€‰ï¼šç‹¬ç«‹æ¨¡å‹ç›®å½•
                    Path('kokoro-models/Kokoro-82M'),
                ])

                found_model = None
                for model_path in local_model_paths:
                    if model_path.exists() and model_path.is_dir():
                        found_model = model_path
                        print(f"âœ… Found local model at: {model_path}", file=sys.stderr)
                        break

                if not found_model:
                    # æ¨¡å‹æœªæ‰¾åˆ°ï¼ŒæŠ›å‡ºæ˜ç¡®é”™è¯¯
                    error_msg = (
                        "âŒ Kokoro model not found in offline mode.\n"
                        "Searched paths:\n" +
                        "\n".join([f"  - {p}" for p in local_model_paths if str(p)]) +
                        "\n\nPlease ensure model exists at one of the above paths,\n"
                        "or set KOKORO_LOCAL_MODEL_PATH environment variable."
                    )
                    raise FileNotFoundError(error_msg)

                # ç¡®ä¿æ¨¡å‹åœ¨æ ‡å‡†ç¼“å­˜ä½ç½®ï¼ˆKokoro æœŸæœ›çš„ç»“æ„ï¼‰
                cache_path = Path.home() / '.cache/huggingface/hub/models--hexgrad--Kokoro-82M'
                snapshot_dir = cache_path / 'snapshots/main'

                if not snapshot_dir.exists() and found_model != snapshot_dir:
                    print(f"ğŸ“¦ Copying model to HuggingFace cache structure...", file=sys.stderr)
                    cache_path.mkdir(parents=True, exist_ok=True)
                    (cache_path / 'refs').mkdir(exist_ok=True)
                    shutil.copytree(found_model, snapshot_dir, dirs_exist_ok=True)
                    (cache_path / 'refs/main').write_text('main')
                    print("âœ… Model copied to cache", file=sys.stderr)

                # åˆå§‹åŒ– pipelineï¼ˆä»…ç¦»çº¿æ¨¡å¼ï¼‰
                try:
                    self.pipeline = KPipeline(lang_code=self.lang_code)
                    print(f"âœ… Kokoro TTS initialized in offline mode for language: {self.lang_code}", file=sys.stderr)
                except Exception as e:
                    print(f"âŒ Pipeline initialization failed: {e}", file=sys.stderr)
                    raise RuntimeError(f"Failed to initialize Kokoro pipeline: {e}")

                self.current_lang_code = self.lang_code
            
            # åŠ è½½è¯­éŸ³ (å¦‚æœè¯­éŸ³æ”¹å˜éœ€è¦é‡æ–°åŠ è½½)
            if self.current_voice != self.voice:
                try:
                    print(f"ğŸµ Loading voice: {self.voice}", file=sys.stderr)
                    self.voice_pack = self.pipeline.load_voice(self.voice)
                    self.current_voice = self.voice
                except Exception as e:
                    print(f"âš ï¸  Warning: Failed to load voice {self.voice}: {e}", file=sys.stderr)
                    # å°è¯•åŠ è½½é»˜è®¤è¯­éŸ³
                    try:
                        fallback_voice = 'af_heart' if self.lang_code == 'a' else f"{self.lang_code}f_alpha"
                        print(f"ğŸ”„ Trying fallback voice: {fallback_voice}", file=sys.stderr)
                        self.voice_pack = self.pipeline.load_voice(fallback_voice)
                        self.voice = fallback_voice
                        self.current_voice = fallback_voice
                    except Exception as e2:
                        print(f"âŒ Fallback voice also failed: {e2}", file=sys.stderr)
                        self.voice_pack = None
            
            self.initialized = True
            
            # å‘é€å°±ç»ªä¿¡å·åˆ°stderr
            print(f'ğŸš€ Kokoro TTS service ready - Language: {self.lang_code}, Voice: {self.voice}', file=sys.stderr)
            sys.stderr.flush()
            
        except Exception as e:
            if self._handle_cuda_initialization_failure(e):
                return await self.initialize(lang_code=self.lang_code, voice=self.voice)
            print(f"âŒ Initialization failed: {e}", file=sys.stderr)
            raise
            
    async def generate_speech(self, text: str, speed: float = 1.0, parallel: bool = True) -> str:
        """ç”ŸæˆéŸ³é¢‘ï¼Œè¿”å›åå…­è¿›åˆ¶å­—ç¬¦ä¸²"""
        if not self.initialized:
            await self.initialize()
            
        if not text or not text.strip():
            raise ValueError("Text cannot be empty")
        
        # å¤„ç†é•¿æ–‡æœ¬ï¼šåˆ†å—è€Œä¸æ˜¯æˆªå–
        print(f"ğŸ” Text length check: {len(text)} chars", file=sys.stderr)
        if len(text) > MAX_CHUNK_CHAR_SIZE:
            print(f"ğŸ“ Text is long ({len(text)} chars), will chunk into {MAX_CHUNK_CHAR_SIZE}-char pieces", file=sys.stderr)
            return await self.generate_speech_chunked(text, speed)
            
        # å¯¹äºçŸ­æ–‡æœ¬ï¼Œç›´æ¥ä½¿ç”¨å•å—å¤„ç†
        return await self.generate_speech_single(text, speed)

    async def generate_speech_chunked(self, text: str, speed: float = 1.0) -> str:
        """åˆ†å—ç”ŸæˆéŸ³é¢‘å¹¶æ‹¼æ¥ï¼ˆçº¿ç¨‹æ± å¹¶è¡Œ + å¯é…ç½®å¹¶å‘åº¦ï¼‰"""
        chunks = split_text_intelligently(text)
        total = len(chunks)
        print(f"ğŸ§© Split text into {total} chunks for parallel processing", file=sys.stderr)

        # åœ¨æœ¬å‡½æ•°å†…å®šä¹‰åŒæ­¥çš„å—ç”Ÿæˆå‡½æ•°ï¼Œåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œ
        def _generate_chunk_hex_sync(chunk_text: str, s: float) -> str:
            import torch
            buffer = io.BytesIO()
            audio_tensors = []
            # ç›´æ¥å¯¹è¯¥å—è¿è¡Œä¸€æ¬¡pipelineï¼ˆé¿å…é‡å¤éå†ï¼‰
            for _, _, audio in self.pipeline(
                chunk_text,
                voice=self.voice,
                speed=s,
                split_pattern=r'\n+'
            ):
                if audio is not None:
                    if self.device == 'mps':
                        audio = audio.to('cpu')
                    audio_tensors.append(audio)

            if not audio_tensors:
                raise Exception("No audio chunks were generated")

            combined = torch.cat(audio_tensors)
            sf.write(buffer, combined.numpy(), 24000, format='WAV')
            return buffer.getvalue().hex()

        # å¹¶å‘åº¦ï¼ˆé»˜è®¤2ï¼Œå¯é€šè¿‡ç¯å¢ƒå˜é‡è°ƒæ•´ï¼‰
        try:
            max_concurrency = max(1, int(os.environ.get('KOKORO_TTS_MAX_CONCURRENCY', '2')))
        except Exception:
            max_concurrency = 2

        sem = asyncio.Semaphore(max_concurrency)

        async def worker(idx: int, chunk_text: str):
            print(f"ğŸš€ Starting chunk {idx+1}/{total}: {len(chunk_text)} chars", file=sys.stderr)
            async with sem:
                # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒCPUå¯†é›†å‹ä»»åŠ¡
                hex_data = await asyncio.to_thread(_generate_chunk_hex_sync, chunk_text, speed)
                return idx, hex_data

        tasks = [worker(i, c) for i, c in enumerate(chunks)]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # å¤„ç†ç»“æœå¹¶æŒ‰åŸå§‹é¡ºåºæ‹¼æ¥
        audio_chunks: list[bytes] = []
        for result in sorted(
            [r for r in results if not isinstance(r, Exception)], key=lambda x: x[0]
        ):
            idx, chunk_hex = result
            try:
                chunk_bytes = bytes.fromhex(chunk_hex)
                if chunk_bytes.startswith(b'RIFF'):
                    data_pos = chunk_bytes.find(b'data')
                    if data_pos == -1:
                        print(f"âŒ No 'data' chunk found in audio for chunk {idx+1}", file=sys.stderr)
                        continue
                    data_start = data_pos + 8
                    if data_start >= len(chunk_bytes):
                        print(f"âŒ Invalid data chunk position for chunk {idx+1}", file=sys.stderr)
                        continue
                    audio_data = chunk_bytes[data_start:]
                    audio_chunks.append(audio_data)
                else:
                    audio_chunks.append(chunk_bytes)
                print(f"âœ… Chunk {idx+1} processed successfully.", file=sys.stderr)
            except Exception as e:
                print(f"âŒ Error processing result for chunk {idx+1}: {e}", file=sys.stderr)
                continue

        
        if not audio_chunks:
            raise Exception("No audio chunks were generated successfully")
        
        # æ‹¼æ¥æ‰€æœ‰éŸ³é¢‘æ•°æ®
        combined_audio_data = b''.join(audio_chunks)
        
        # ç¡®ä¿éŸ³é¢‘æ•°æ®å¯¹é½åˆ°æ ·æœ¬è¾¹ç•Œ (16ä½=2å­—èŠ‚å¯¹é½)
        if len(combined_audio_data) % 2 != 0:
            combined_audio_data += b'\x00'  # å¡«å……ä¸€ä¸ªå­—èŠ‚
        
        # åˆ›å»ºæ–°çš„WAVå¤´
        import struct
        sample_rate = 24000
        num_channels = 1
        bits_per_sample = 16
        byte_rate = sample_rate * num_channels * bits_per_sample // 8
        block_align = num_channels * bits_per_sample // 8
        data_size = len(combined_audio_data)
        file_size = 36 + data_size
        
        # ç¡®ä¿WAVå¤´å®Œå…¨æ­£ç¡® (RIFFæ–‡ä»¶å¤§å°åº”è¯¥æ˜¯æ•´ä¸ªæ–‡ä»¶å‡å»8å­—èŠ‚)
        riff_size = 36 + data_size  # fmt chunk (24 bytes) + data chunk header (8 bytes) + data
        wav_header = struct.pack('<4sI4s4sIHHIIHH4sI',
            b'RIFF',           # ChunkID
            riff_size,         # ChunkSize (æ•´ä¸ªæ–‡ä»¶å¤§å° - 8)
            b'WAVE',           # Format
            b'fmt ',           # Subchunk1ID
            16,                # Subchunk1Size (PCM = 16)
            1,                 # AudioFormat (PCM = 1)
            num_channels,      # NumChannels
            sample_rate,       # SampleRate
            byte_rate,         # ByteRate
            block_align,       # BlockAlign
            bits_per_sample,   # BitsPerSample
            b'data',           # Subchunk2ID
            data_size          # Subchunk2Size
        )
        
        print(f"ğŸ”§ WAV Header Info:", file=sys.stderr)
        print(f"  - Sample Rate: {sample_rate} Hz", file=sys.stderr)
        print(f"  - Channels: {num_channels}", file=sys.stderr)
        print(f"  - Bit Depth: {bits_per_sample} bits", file=sys.stderr)
        print(f"  - Data Size: {data_size} bytes", file=sys.stderr)
        print(f"  - File Size: {file_size + 8} bytes", file=sys.stderr)
        
        # ç»„åˆå®Œæ•´çš„WAVæ–‡ä»¶
        complete_wav = wav_header + combined_audio_data
        
        total_duration = len(combined_audio_data) / (sample_rate * num_channels * bits_per_sample // 8)
        print(f"ğŸµ Combined {len(chunks)} chunks, total length: {total_duration:.2f}s", file=sys.stderr)
        
        return complete_wav.hex()
    
    async def generate_speech_single(self, text: str, speed: float = 1.0) -> str:
        """ç”Ÿæˆå•ä¸ªæ–‡æœ¬å—çš„éŸ³é¢‘ï¼ˆåŸæ¥çš„generate_speeché€»è¾‘ï¼‰"""
        try:
            import torch
            import concurrent.futures
            
            # é¦–å…ˆæ”¶é›†æ‰€æœ‰æ–‡æœ¬å—
            text_chunks = []
            for i, (graphemes, phonemes, audio) in enumerate(
                self.pipeline(
                    text, 
                    voice=self.voice, 
                    speed=speed,
                    split_pattern=r'\n+'
                )
            ):
                if audio is not None:
                    text_chunks.append((i, graphemes, phonemes))
                    print(f"ğŸµ Found chunk {i+1}: {len(graphemes)} chars, {len(phonemes)} phonemes", file=sys.stderr)
            
            if not text_chunks:
                raise Exception("No audio chunks were generated")
            
            print(f"ğŸµ Total {len(text_chunks)} chunks to process", file=sys.stderr)
            
            # ä¸²è¡Œå¤„ç†ï¼ˆå•å—éŸ³é¢‘ä¸éœ€è¦å¹¶è¡Œï¼‰
            print(f"ğŸ“ Processing {len(text_chunks)} chunks sequentially...", file=sys.stderr)
            
            audio_chunks = []
            for i, (graphemes, phonemes, audio) in enumerate(
                self.pipeline(
                    text, 
                    voice=self.voice, 
                    speed=speed,
                    split_pattern=r'\n+'
                )
            ):
                if audio is not None:
                    if self.device == 'mps':
                        audio = audio.to('cpu')
                    audio_chunks.append(audio)
                    print(f"âœ… Chunk {i+1} completed", file=sys.stderr)
            
            if not audio_chunks:
                raise Exception("No audio chunks were generated successfully")
            
            # æ‹¼æ¥æ‰€æœ‰éŸ³é¢‘å—
            combined_audio = torch.cat(audio_chunks)
            
            # è½¬æ¢ä¸ºå­—èŠ‚æ•°æ®
            import io
            import soundfile as sf
            buffer = io.BytesIO()
            sf.write(buffer, combined_audio.numpy(), 24000, format='WAV')
            audio_data = buffer.getvalue()
            
            total_duration = len(combined_audio) / 24000
            print(f"ğŸµ Generated {len(audio_chunks)} chunks, total length: {total_duration:.2f}s", file=sys.stderr)
            
            return audio_data.hex()
            
        except Exception as e:
            print(f"âŒ Error in generate_speech_single: {e}", file=sys.stderr)
            import traceback
            traceback.print_exc()
            raise
    
    def _log_known_warning_context(self) -> None:
        """æç¤ºç”¨æˆ·ä¸Šæ¸¸PyTorchçš„å·²çŸ¥è­¦å‘Šå±äºé¢„æœŸè¡Œä¸º"""
        if self._warned_about_known_warnings:
            return
        self._warned_about_known_warnings = True
        print(
            "â„¹ï¸  Kokoro uses upstream GRU/weight_norm layers that may trigger one-time PyTorch warnings. "
            "They are benign and do not block model loading.",
            file=sys.stderr
        )

    def _is_cuda_arch_supported(self) -> bool:
        """æ£€æµ‹å½“å‰CUDAè®¾å¤‡æ˜¯å¦è¢«å½“å‰PyTorchæ„å»ºæ”¯æŒ"""
        try:
            capability = torch.cuda.get_device_capability(0)
            arch_tag = f"sm_{capability[0]}{capability[1]}"
            arch_list = getattr(torch.cuda, "get_arch_list", lambda: [])()
            if arch_list and arch_tag not in arch_list:
                print(
                    f"âš ï¸  CUDA device with capability {arch_tag} is not in this PyTorch build "
                    f"(available: {', '.join(arch_list)}).",
                    file=sys.stderr
                )
                return False
            return True
        except Exception as exc:
            print(f"âš ï¸  Unable to verify CUDA architecture compatibility: {exc}", file=sys.stderr)
            return True

    def _handle_cuda_initialization_failure(self, error: Exception) -> bool:
        """åœ¨CUDAåˆå§‹åŒ–å¤±è´¥æ—¶å°è¯•é™çº§åˆ°CPUï¼Œè¿”å›æ˜¯å¦å·²å¤„ç†"""
        if self.device != 'cuda' or self._gpu_fallback_attempted:
            return False

        error_text = str(error)
        known_markers = [
            "no kernel image is available",
            "is not compatible with the current PyTorch installation",
            "SM version",
            "CUDA error"
        ]
        if any(marker in error_text for marker in known_markers):
            print(f"âš ï¸  CUDA initialization failed due to: {error_text}", file=sys.stderr)
            print("ğŸ”„ Switching Kokoro TTS execution to CPU to keep the service available.", file=sys.stderr)
            self._gpu_fallback_attempted = True
            self.device = 'cpu'
            try:
                if hasattr(torch.cuda, "empty_cache"):
                    torch.cuda.empty_cache()
            except Exception:
                pass
            # é‡ç½®çŠ¶æ€ä»¥ä¾¿åœ¨CPUæ¨¡å¼ä¸‹é‡æ–°åˆå§‹åŒ–
            self.pipeline = None
            self.voice_pack = None
            self.initialized = False
            self.current_lang_code = None
            self.current_voice = None
            return True

        return False

async def main():
    """ä¸»å‡½æ•°ï¼šç›‘å¬stdinè¾“å…¥å¹¶å¤„ç†è¯·æ±‚"""
    service = KokoroTTSWrapper()
    
    try:
        # åˆå§‹åŒ–æ¨¡å‹
        await service.initialize()
        
        # å‘é€å°±ç»ªä¿¡å·åˆ°stderr
        print('ğŸš€ Kokoro TTS service is ready', file=sys.stderr)
        sys.stderr.flush()
        
        # ç›‘å¬æ ‡å‡†è¾“å…¥
        while True:
            try:
                line = sys.stdin.readline()
                if not line:
                    break
                    
                line = line.strip()
                if not line:
                    continue
                    
                # è§£æJSONè¯·æ±‚
                request = json.loads(line)
                
                # è·å–è¯­è¨€å’Œè¯­éŸ³é…ç½®
                lang_code = request.get('lang_code', 'a')
                voice = request.get('voice', 'af_heart')
                
                # é‡æ–°åˆå§‹åŒ–æœåŠ¡ï¼ˆå¦‚æœéœ€è¦ï¼‰
                await service.initialize(lang_code=lang_code, voice=voice)
                
                # å¤„ç†è¯·æ±‚
                result = await service.generate_speech(
                    text=request.get('text', ''),
                    speed=float(request.get('speed', 1.0))
                )
                
                # è¿”å›ç»“æœ
                response = {
                    "success": True,
                    "audio_data": result,
                    "device": service.device or "unknown",
                    "lang_code": service.lang_code,
                    "voice": service.voice,
                    "message": "Audio generated successfully"
                }
                
                # å‘é€JSONå“åº”
                json_response = json.dumps(response)
                print(json_response, flush=True)
                
            except json.JSONDecodeError:
                error_response = {
                    "success": False,
                    "error": "Invalid JSON format"
                }
                print(json.dumps(error_response))
                sys.stdout.flush()
                
            except Exception as e:
                error_response = {
                    "success": False,
                    "error": str(e)
                }
                print(json.dumps(error_response))
                sys.stdout.flush()
                
    except KeyboardInterrupt:
        pass
    except Exception as e:
        sys.exit(1)

# ç®€å•æµ‹è¯•å‡½æ•°
async def test():
    """æµ‹è¯•å‡½æ•°"""
    service = KokoroTTSWrapper()
    await service.initialize()
    result = await service.generate_speech("Hello test", 1.0)
    print(json.dumps({
        "success": True,
        "audio_data": result,
        "device": service.device or "unknown",
        "message": "Test completed"
    }))

if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1 and sys.argv[1] == "--test":
        asyncio.run(test())
    else:
        asyncio.run(main())
