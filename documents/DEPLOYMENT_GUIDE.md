# è¿œç¨‹æœåŠ¡å™¨éƒ¨ç½²æŒ‡å—

## æœåŠ¡å™¨ä¿¡æ¯
- **ç³»ç»Ÿ**: Ubuntu
- **IP**: 49.234.30.246
- **ç«¯å£**: 60022
- **ç”¨æˆ·**: ubuntu
- **å¯†ç **: Abcd.1234
- **GPU**: NVIDIA Tesla P40

## éƒ¨ç½²ç­–ç•¥

### æ–¹æ¡ˆé€‰æ‹©
ç”±äºæœåŠ¡å™¨å·²æœ‰æ—§ç‰ˆæœ¬é¡¹ç›®ï¼Œå»ºè®®é‡‡ç”¨**æ›´æ–°éƒ¨ç½²**è€Œéåˆ é™¤é‡å»ºï¼š
1. ä¿ç•™æ•°æ®åº“å’Œç”¨æˆ·æ•°æ®
2. æ›´æ–°ä»£ç å’Œä¾èµ–
3. é‡æ–°æ„å»ºå’Œå¯åŠ¨æœåŠ¡

## å‰ç½®å‡†å¤‡

### 1. æœ¬åœ°æäº¤ä»£ç 
```bash
# åœ¨æœ¬åœ°æ‰§è¡Œ
git add .
git commit -m "å‡†å¤‡éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ"
git push origin main  # æˆ–ä½ çš„åˆ†æ”¯å
```

### 2. è¿æ¥åˆ°æœåŠ¡å™¨
```bash
ssh -p 60022 ubuntu@49.234.30.246
```

## éƒ¨ç½²æ­¥éª¤

### æ­¥éª¤ 1: æ£€æŸ¥æœåŠ¡å™¨ç¯å¢ƒ

```bash
# æ£€æŸ¥ GPU é©±åŠ¨
nvidia-smi

# æ£€æŸ¥ Docker
docker --version
docker compose version

# æ£€æŸ¥ NVIDIA Container Toolkit
docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi

# æ£€æŸ¥ Python
python3 --version
```

### æ­¥éª¤ 2: è¿›å…¥é¡¹ç›®ç›®å½•å¹¶å¤‡ä»½

```bash
# è¿›å…¥é¡¹ç›®ç›®å½•ï¼ˆæ ¹æ®å®é™…è·¯å¾„è°ƒæ•´ï¼‰
cd /path/to/your/project

# åˆ›å»ºå¤‡ä»½
./scripts/backup.sh --compress

# æˆ–æ‰‹åŠ¨å¤‡ä»½æ•°æ®åº“
cp data/app.db data/app.db.backup.$(date +%Y%m%d_%H%M%S)
```

### æ­¥éª¤ 3: æ‹‰å–æœ€æ–°ä»£ç 

```bash
# åœæ­¢å½“å‰è¿è¡Œçš„æœåŠ¡
docker compose -f docker-compose.gpu.yml down

# æˆ–å¦‚æœä½¿ç”¨ PM2
pm2 stop all

# æ‹‰å–æœ€æ–°ä»£ç 
git fetch origin
git pull origin main  # æˆ–ä½ çš„åˆ†æ”¯å

# æŸ¥çœ‹æ›´æ–°å†…å®¹
git log -5 --oneline
```

### æ­¥éª¤ 4: æ£€æŸ¥å’Œé…ç½®ç¯å¢ƒå˜é‡

```bash
# æ£€æŸ¥ .env.production æ–‡ä»¶
cat .env.production

# å¦‚æœä¸å­˜åœ¨ï¼Œä»ç¤ºä¾‹åˆ›å»º
cp .env.production.example .env.production

# ç¼–è¾‘é…ç½®ï¼ˆé‡è¦ï¼ï¼‰
nano .env.production
```

å¿…éœ€çš„ç¯å¢ƒå˜é‡ï¼š
```bash
# Cerebras API
CEREBRAS_API_KEY=your_api_key_here

# JWT å¯†é’¥
JWT_SECRET=your_jwt_secret_here

# æ•°æ®åº“
DATABASE_URL=file:/app/data/app.db

# GPU é…ç½®
KOKORO_DEVICE=cuda
PYTORCH_ENABLE_MPS_FALLBACK=0

# ç®¡ç†å‘˜è´¦å·
ADMIN_EMAIL=admin@example.com
ADMIN_PASSWORD=Admin123456
ADMIN_NAME=System Administrator

# å¯é€‰ï¼šä»£ç†é…ç½®
# CEREBRAS_PROXY_URL=http://your-proxy:port
```

### æ­¥éª¤ 5: æ£€æŸ¥ GPU ç¯å¢ƒ

```bash
# è¿è¡Œ GPU ç¯å¢ƒæ£€æŸ¥è„šæœ¬
PYTHON_BIN=python3 ./scripts/gpu-environment-check.sh
```

å¦‚æœæ£€æŸ¥å¤±è´¥ï¼Œéœ€è¦å®‰è£… NVIDIA Container Toolkitï¼š
```bash
# Ubuntu å®‰è£… NVIDIA Container Toolkit
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \
  sudo tee /etc/apt/sources.list.d/nvidia-docker.list

sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker
```

### æ­¥éª¤ 6: è®¾ç½® Kokoro TTS ç¯å¢ƒ

```bash
# è¿è¡Œå®Œæ•´çš„ Kokoro è®¾ç½®è„šæœ¬
./scripts/setup-kokoro-complete.sh

# æˆ–ä½¿ç”¨ GPU ä¸“ç”¨å®‰è£…è„šæœ¬
./scripts/install-pytorch-gpu.sh --python python3 --cuda-version 11.8
```

### æ­¥éª¤ 7: ä½¿ç”¨ Docker GPU éƒ¨ç½²

#### æ–¹æ¡ˆ A: ä½¿ç”¨é¢„æ„å»ºé•œåƒï¼ˆæ¨èï¼‰

ä½¿ç”¨ GitHub Container Registry (GHCR) çš„é¢„æ„å»ºé•œåƒï¼Œå¿«é€Ÿéƒ¨ç½²ï¼Œæ— éœ€æœ¬åœ°æ„å»ºã€‚

**é¦–æ¬¡è®¾ç½®ï¼š**

```bash
# 1. åˆ›å»º GitHub Personal Access Token (PAT)
# è®¿é—®: GitHub Settings > Developer settings > Personal access tokens > Tokens (classic)
# æƒé™: read:packages

# 2. ç™»å½• GHCRï¼ˆé¦–æ¬¡éœ€è¦ï¼‰
echo $GHCR_TOKEN | docker login ghcr.io -u arthurlee116 --password-stdin

# 3. éªŒè¯ç™»å½•
docker pull ghcr.io/arthurlee116/english-listening-trainer:latest
```

**éƒ¨ç½²æ­¥éª¤ï¼š**

```bash
# ä½¿ç”¨è‡ªåŠ¨åŒ–éƒ¨ç½²è„šæœ¬ï¼ˆæ¨èï¼‰
./scripts/deploy-from-ghcr.sh

# è„šæœ¬ä¼šè‡ªåŠ¨æ‰§è¡Œï¼š
# - æ‹‰å–æœ€æ–°é•œåƒ
# - æ¯”è¾ƒç‰ˆæœ¬ï¼ˆå¦‚æœç›¸åŒåˆ™è·³è¿‡ï¼‰
# - å¤‡ä»½æ•°æ®åº“
# - åœæ­¢æ—§å®¹å™¨
# - å¯åŠ¨æ–°å®¹å™¨
# - å¥åº·æ£€æŸ¥éªŒè¯

# æˆ–æ‰‹åŠ¨æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š

# 1. æ‹‰å–æœ€æ–°é•œåƒ
docker pull ghcr.io/arthurlee116/english-listening-trainer:latest

# 2. å¤‡ä»½æ•°æ®åº“ï¼ˆå¯é€‰ä½†æ¨èï¼‰
./scripts/backup.sh --compress

# 3. è¿è¡Œæ•°æ®åº“è¿ç§»
export IMAGE_TAG=ghcr.io/arthurlee116/english-listening-trainer:latest
docker compose -f docker-compose.gpu.yml run --rm migrate

# 4. å¯åŠ¨åº”ç”¨
docker compose -f docker-compose.gpu.yml up -d app

# 5. æŸ¥çœ‹æ—¥å¿—
docker compose -f docker-compose.gpu.yml logs -f app
```

**éƒ¨ç½²ç‰¹å®šç‰ˆæœ¬ï¼š**

```bash
# éƒ¨ç½²ç‰¹å®š Git commit ç‰ˆæœ¬ï¼ˆç”¨äºå›æ»šï¼‰
./scripts/deploy-from-ghcr.sh main-abc1234

# æˆ–æ‰‹åŠ¨æŒ‡å®šç‰ˆæœ¬
export IMAGE_TAG=ghcr.io/arthurlee116/english-listening-trainer:main-abc1234
docker compose -f docker-compose.gpu.yml up -d
```

#### æ–¹æ¡ˆ B: æœ¬åœ°æ„å»ºï¼ˆå¼€å‘ç¯å¢ƒï¼‰

é€‚ç”¨äºéœ€è¦ä¿®æ”¹ä»£ç æˆ–æµ‹è¯•æœ¬åœ°æ›´æ”¹çš„åœºæ™¯ã€‚

```bash
# ä½¿ç”¨ GPU éƒ¨ç½²è„šæœ¬ï¼ˆæ¨èï¼‰
./scripts/deploy-gpu.sh

# æˆ–æ‰‹åŠ¨æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š

# 1. æ„å»ºé•œåƒ
docker compose -f docker-compose.gpu.yml build app

# 2. è¿è¡Œæ•°æ®åº“è¿ç§»
docker compose -f docker-compose.gpu.yml run --rm migrate

# 3. å¯åŠ¨åº”ç”¨
docker compose -f docker-compose.gpu.yml up -d app

# 4. æŸ¥çœ‹æ—¥å¿—
docker compose -f docker-compose.gpu.yml logs -f app
```

#### æ–¹æ¡ˆå¯¹æ¯”

| ç‰¹æ€§ | æ–¹æ¡ˆ A: é¢„æ„å»ºé•œåƒ | æ–¹æ¡ˆ B: æœ¬åœ°æ„å»º |
|------|------------------|----------------|
| **éƒ¨ç½²é€Ÿåº¦** | âš¡ 2-5 åˆ†é’Ÿ | ğŸŒ 30-60 åˆ†é’Ÿ |
| **ç½‘ç»œè¦æ±‚** | ä½ï¼ˆåªæ‹‰å–æœ€ç»ˆé•œåƒï¼‰| é«˜ï¼ˆéœ€ä¸‹è½½ CUDA åŸºç¡€é•œåƒï¼‰|
| **å¯é æ€§** | âœ… é«˜ï¼ˆGitHub åŸºç¡€è®¾æ–½ï¼‰| âš ï¸ ä¸­ï¼ˆç½‘ç»œä¸ç¨³å®šå¯èƒ½å¤±è´¥ï¼‰|
| **ç£ç›˜ç©ºé—´** | èŠ‚çœï¼ˆæ— éœ€æ„å»ºç¼“å­˜ï¼‰| å ç”¨å¤§ï¼ˆéœ€è¦æ„å»ºå±‚ç¼“å­˜ï¼‰|
| **é€‚ç”¨åœºæ™¯** | ç”Ÿäº§éƒ¨ç½²ã€å¿«é€Ÿæ›´æ–° | å¼€å‘è°ƒè¯•ã€æœ¬åœ°ä¿®æ”¹ |
| **ç‰ˆæœ¬ç®¡ç†** | âœ… è‡ªåŠ¨ï¼ˆGit SHA æ ‡ç­¾ï¼‰| âš ï¸ æ‰‹åŠ¨ç®¡ç† |
| **å›æ»šèƒ½åŠ›** | âœ… ç®€å•ï¼ˆæŒ‡å®šç‰ˆæœ¬æ ‡ç­¾ï¼‰| âš ï¸ å›°éš¾ï¼ˆéœ€è¦é‡æ–°æ„å»ºï¼‰|
| **æ„å»ºç¼“å­˜** | âœ… GitHub è‡ªåŠ¨ç®¡ç† | âš ï¸ æœ¬åœ°æ‰‹åŠ¨ç®¡ç† |

**æ¨èä½¿ç”¨åœºæ™¯ï¼š**
- **ç”Ÿäº§ç¯å¢ƒ**: ä½¿ç”¨æ–¹æ¡ˆ Aï¼ˆé¢„æ„å»ºé•œåƒï¼‰
- **å¼€å‘ç¯å¢ƒ**: ä½¿ç”¨æ–¹æ¡ˆ Bï¼ˆæœ¬åœ°æ„å»ºï¼‰
- **ç´§æ€¥ä¿®å¤**: ä½¿ç”¨æ–¹æ¡ˆ A å¿«é€Ÿå›æ»š
- **åŠŸèƒ½æµ‹è¯•**: ä½¿ç”¨æ–¹æ¡ˆ B æµ‹è¯•æœ¬åœ°æ›´æ”¹

### æ­¥éª¤ 8: éªŒè¯éƒ¨ç½²

```bash
# è¿è¡Œå¥åº·æ£€æŸ¥
./scripts/health-check.sh

# æˆ–æ‰‹åŠ¨æ£€æŸ¥
curl http://localhost:3000/api/health

# æ£€æŸ¥å®¹å™¨çŠ¶æ€
docker compose -f docker-compose.gpu.yml ps

# æŸ¥çœ‹ GPU ä½¿ç”¨æƒ…å†µ
nvidia-smi

# æµ‹è¯• TTS åŠŸèƒ½
curl -X POST http://localhost:3000/api/tts \
  -H "Content-Type: application/json" \
  -d '{"text":"Hello world","voice":"af_heart"}'
```

### æ­¥éª¤ 9: å¯åŠ¨ç®¡ç†åå°ï¼ˆå¯é€‰ï¼‰

```bash
# å¯åŠ¨ç®¡ç†åå°ï¼ˆç«¯å£ 3005ï¼‰
docker compose -f docker-compose.gpu.yml --profile admin up -d admin

# è®¿é—®ç®¡ç†åå°
# http://your-server-ip:3005
```

## æ•…éšœæ’æŸ¥

### é—®é¢˜ 1: GPU ä¸å¯ç”¨

```bash
# æ£€æŸ¥ NVIDIA é©±åŠ¨
nvidia-smi

# æ£€æŸ¥ Docker GPU æ”¯æŒ
docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi

# é‡å¯ Docker
sudo systemctl restart docker
```

### é—®é¢˜ 2: TTS åˆå§‹åŒ–å¤±è´¥

```bash
# æ£€æŸ¥ Python ç¯å¢ƒ
cd kokoro-local
source venv/bin/activate
python -c "import torch; print(torch.cuda.is_available())"

# é‡æ–°å®‰è£… PyTorch
./scripts/install-pytorch-gpu.sh --recreate --cuda-version 11.8
```

### é—®é¢˜ 3: æ•°æ®åº“è¿ç§»å¤±è´¥

```bash
# æ‰‹åŠ¨è¿è¡Œè¿ç§»
docker compose -f docker-compose.gpu.yml run --rm app npx prisma migrate deploy

# æˆ–è¿›å…¥å®¹å™¨
docker compose -f docker-compose.gpu.yml exec app sh
npx prisma migrate deploy
```

### é—®é¢˜ 4: ç«¯å£è¢«å ç”¨

```bash
# æ£€æŸ¥ç«¯å£å ç”¨
sudo lsof -i :3000
sudo lsof -i :3005

# åœæ­¢å ç”¨ç«¯å£çš„è¿›ç¨‹
sudo kill -9 <PID>
```

## å›æ»šæ“ä½œ

å¦‚æœéƒ¨ç½²å‡ºç°é—®é¢˜ï¼Œå¯ä»¥å¿«é€Ÿå›æ»šï¼š

```bash
# ä½¿ç”¨éƒ¨ç½²è„šæœ¬å›æ»š
./scripts/deploy.sh --rollback deploy_YYYYMMDD_HHMMSS

# æˆ–æ‰‹åŠ¨å›æ»š
git log --oneline  # æŸ¥çœ‹æäº¤å†å²
git checkout <commit-hash>
docker compose -f docker-compose.gpu.yml down
docker compose -f docker-compose.gpu.yml up -d --build
```

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### GPU å†…å­˜ä¼˜åŒ–
```bash
# åœ¨ .env.production ä¸­è®¾ç½®
KOKORO_TTS_MAX_CONCURRENCY=4  # Tesla P40 æœ‰ 24GB æ˜¾å­˜ï¼Œå¯ä»¥å¢åŠ å¹¶å‘
```

### æ—¥å¿—ç®¡ç†
```bash
# å®šæœŸæ¸…ç†æ—¥å¿—
find logs/ -name "*.log" -mtime +30 -delete

# æ¸…ç†æ—§çš„éŸ³é¢‘æ–‡ä»¶
find public/audio/ -name "*.wav" -mtime +7 -delete
```

## ç›‘æ§å’Œç»´æŠ¤

### æŸ¥çœ‹åº”ç”¨çŠ¶æ€
```bash
# Docker æ–¹å¼
docker compose -f docker-compose.gpu.yml ps
docker compose -f docker-compose.gpu.yml logs -f app

# æŸ¥çœ‹èµ„æºä½¿ç”¨
docker stats

# æŸ¥çœ‹ GPU ä½¿ç”¨
watch -n 1 nvidia-smi
```

### å®šæœŸå¤‡ä»½
```bash
# æ·»åŠ åˆ° crontab
crontab -e

# æ¯å¤©å‡Œæ™¨ 2 ç‚¹å¤‡ä»½
0 2 * * * cd /path/to/project && ./scripts/backup.sh --compress
```

## å¸¸ç”¨å‘½ä»¤é€ŸæŸ¥

```bash
# å¯åŠ¨æœåŠ¡
docker compose -f docker-compose.gpu.yml up -d

# åœæ­¢æœåŠ¡
docker compose -f docker-compose.gpu.yml down

# é‡å¯æœåŠ¡
docker compose -f docker-compose.gpu.yml restart

# æŸ¥çœ‹æ—¥å¿—
docker compose -f docker-compose.gpu.yml logs -f

# è¿›å…¥å®¹å™¨
docker compose -f docker-compose.gpu.yml exec app sh

# é‡æ–°æ„å»º
docker compose -f docker-compose.gpu.yml up -d --build

# æ¸…ç†æœªä½¿ç”¨çš„é•œåƒ
docker system prune -a
```

## å®‰å…¨å»ºè®®

1. **ä¿®æ”¹é»˜è®¤å¯†ç **: éƒ¨ç½²åç«‹å³ä¿®æ”¹ç®¡ç†å‘˜å¯†ç 
2. **é…ç½®é˜²ç«å¢™**: åªå¼€æ”¾å¿…è¦çš„ç«¯å£ï¼ˆ3000, 3005ï¼‰
3. **ä½¿ç”¨ HTTPS**: é…ç½® Nginx åå‘ä»£ç†å’Œ SSL è¯ä¹¦
4. **å®šæœŸæ›´æ–°**: ä¿æŒç³»ç»Ÿå’Œä¾èµ–åŒ…æ›´æ–°
5. **å¤‡ä»½ç­–ç•¥**: å®šæœŸå¤‡ä»½æ•°æ®åº“å’Œé‡è¦æ–‡ä»¶

## ä¸‹ä¸€æ­¥

éƒ¨ç½²å®Œæˆåï¼š
1. è®¿é—®åº”ç”¨: `http://49.234.30.246:3000`
2. è®¿é—®ç®¡ç†åå°: `http://49.234.30.246:3005`
3. ä½¿ç”¨ç®¡ç†å‘˜è´¦å·ç™»å½•
4. æµ‹è¯• TTS åŠŸèƒ½
5. é…ç½® Nginx åå‘ä»£ç†ï¼ˆæ¨èï¼‰
